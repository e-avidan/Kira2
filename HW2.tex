\documentclass{article}      
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{ifthen}
\usepackage[T1]{fontenc}

\begin{document}

\title{%
  236605: Assignment \#2 \\
  \large RNN for NER and LM \\
    Fall Semester '17-'18}

\author{
  Avidan, Eyal \\
  \texttt{205796469}
  \and
  Goaz, Or \\
  \texttt{307950113}
}

\maketitle

\section*{1. Softmax}
\subsection*{C. Explanation of TensorFlow Components}
TensorFlow defines two objects used in the \textbf{Model} class
\begin{itemize}
\item \emph{Placeholder Variables} - nodes that expect a value to be provided from the outside - Inputs!
\item \emph{Feed Dictionaries} - a dictionary of data, provided when running a session over the graph, that defines the values of \emph{Placeholder Variables}
\end{itemize}

\subsection*{E. Why we like \emph{TensorFlow} and its automatic differentiation}
Since TensorFlow performs differentation automatically (probably similarly to the way we checked the gradient in the previous exercise), we don't need to specify them - and furthermore, we don't have to calculate them for every change we make to our model, allowing us to play with the computational graph at ease


\section*{2. Deep Networks for \textbf{NER}}
\subsection*{A. Gradients}
Before delving into the gradients of the network, we'll calculate the gradient of the activation function by its parameter
$$
	\frac {\partial tanh(z)} {\partial z} = 2 \cdot \frac {\partial} {\partial z} \cdot \sigma (2z) = 4 \cdot \sigma(2z)\cdot (1- \sigma(2z))
$$

Playinng a bit with the numbers, we can acheive
$$
	[ 2 \cdot \sigma(2z)] \cdot [2 - 2\sigma(2z)]= [(2\sigma(2z) -1) + 1] \cdot -[(2\sigma(2z) -1) - 1] = -[tanh(z) + 1][tanh(z)-1] = -(tanh^2(z) - 1)
$$

Meaning that
$$
	\frac {\partial tanh(z)} {\partial z} = 1 - tanh^2(z)
$$

In addition to this, we will note some facts and notations:
\begin{itemize}
\item When using cross-entropy cost and a softmax activation, the gradient of the cost function by the softmax parameter is $ \frac {\partial J}{\partial \theta} = \hat y - y$ (Proven in HW1)
\item We will notate the linear inputs of the activation functions as $$ z_2 = x^{(t)}\cdot W + b_1 $$ $$ z_3 = h \cdot U + b_2$$
\end{itemize}

\subsubsection*{Output Layer}
$$
	\frac {\partial J}{\partial b_2} = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial b_2} = (\hat y - y) \cdot 1 = \hat y - y = \delta_{(3)}
$$

$$
	\frac {\partial J}{\partial U} = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial U} = (\hat y - y) \otimes h = \delta_{(3)} \otimes h
$$

\subsubsection*{Hidden Layer}
$$
	\frac {\partial J}{\partial b_1} = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial h} \cdot \frac {\partial h}{\partial z_2} \cdot \frac {\partial z_2}{\partial b_1} = \delta_{(3)} \cdot U^{T} \cdot tanh^{'}(z_2) \cdot 1= \delta_{(3)} \cdot U^{T} \cdot tanh^{'}(z_2) = \delta_{(2)}
$$

$$
	\frac {\partial J}{\partial W} = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial W} = \delta_{(2)} \otimes {x^{(t)}}^{T}
$$

\subsubsection*{Words}
?????????????????
$$
	\frac {\partial J}{\partial L_i} = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial L_i} = \delta_{(2)} \cdot {W}^{T}
$$

\subsection*{B. Gradients, with Regularization}
Since they don't appear in the regularization factor,
$$
	\frac {\partial J_{reg}}{\partial L_i} = \frac {\partial J_{reg}}{\partial b_2}  = \frac {\partial J_{reg}}{\partial b_1}  = 0
$$

So their gradients remain the same. \\~\\
As for the weights

$$
	\frac {\partial J_{full}}{\partial U} = \frac {\partial J}{\partial U} + \frac {\partial J_{reg}}{\partial U} = \frac {\partial J}{\partial U} +  \lambda \cdot U
$$

$$
	\frac {\partial J_{full}}{\partial W} = \frac {\partial J}{\partial W} + \frac {\partial J_{reg}}{\partial W} = \frac {\partial J}{\partial W} +  \lambda \cdot W
$$

\subsection*{C. Results}
\subsubsection*{Test Set Labels}
In the file `q2\_test.final.predicted'

\subsubsection*{Hyperparameter Analysis}
Quite a few things were apparent while testing different combinations of hyperparameters:
\begin{itemize}
\item Since most labels were objects, the model easily converged to a local minima with the default \emph{learning rate}, so we have to increase it (but not too much)
\item There weren't so much data, so in order to prevent overfitting we tempered with the hidden layer size (less parameters to learn)
\item Dropout was very important, as it made the network perform similarly on test and dev
\end{itemize}

First, the `mathematically" best configuration was
\begin{lstlisting}
  embed_size = 50
  batch_size = 64
  label_size = 5
  hidden_size = 100
  max_epochs = 48
  early_stopping = 3
  dropout = 0.9
  lr = 0.01
  l2 = 0.001
  window_size = 3
\end{lstlisting}

which led to results

\begin{lstlisting}
Training loss: 7.18358150742e-11
Training acc: 0.832787384405
Validation loss: 8.16334153053e-11

[[42759     0     0     0     0]
 [ 2094     0     0     0     0]
 [ 1268     0     0     0     0]
 [ 2092     0     0     0     0]
 [ 3149     0     0     0     0]]
Tag: O - P 0.8325 / R 1.0000
Tag: LOC - P nan / R 0.0000
Tag: MISC - P nan / R 0.0000
Tag: ORG - P nan / R 0.0000
Tag: PER - P nan / R 0.0000
\end{lstlisting}

As you can see, loss is very low, and both data sets have a similar magnitude of loss, hopefully meaning the model is not overfitted to the test set. However, we do belive it is overfitted to both sets, as it only predicts objects (since most tags are objects). \\~\\
By increasing the learning rate to $lr=0.01$, we belive to have acheived better (more generic) results
\begin{lstlisting}
Training loss: 1.22144655279e-07
Training acc: 0.441334636408
Validation loss: 1.58630811598e-07

[[16750  1999 17753     1  6256]
 [  581    48   765     0   700]
 [  413    30   576     0   249]
 [  896    16   703     0   477]
 [ 1786     5  1111     0   247]]
Tag: O - P 0.8200 / R 0.3917
Tag: LOC - P 0.0229 / R 0.0229
Tag: MISC - P 0.0275 / R 0.4543
Tag: ORG - P 0.0000 / R 0.0000
Tag: PER - P 0.0312 / R 0.0784
\end{lstlisting}
 
The error is larger, but reduced training accuracy combined with more diverse labels leads us to believe this to be the better model configuration.

\section*{3. Recurrent Neural Networks for \emph{LM}}
\subsection*{A. Perplexity}
\subsubsection*{Deriviation from Cross-Entropy}

\(y^{(t)}\)  is one-hot vector. Let's define \(y_i^{(t)}\) the nonzero value in \(y^{(t)}\) . Now, we define CE and PP.

$$
CE(y^{(t)} , \hat y^{(t)} ) = -log(\hat y_i^{(t)}) = log \left(\frac {1} {\hat y_i^{(t)}}\right) \\
PP^{(t)}(y^{(t)} , \hat y^{(t)} ) = \frac {1} {\hat y_i^{(t)}}
$$
Now, we will define Cross-Entropy using Perplexity:
$$
CE(y^{(t)} , \hat y^{(t)} ) = log (PP^{(t)}(y^{(t)} , \hat y^{(t)} ))
$$

This equation means that minimizing the arithmetic mean of the Cross-Entropy and minimizing the geometric mean of the Perplexity is the same.\\

Thus, meanimizing the mean-cross entropy will also minimize the Perplexity, since we only log of it, which is a monotone operation.

\subsubsection*{Cross-Entropy loss and Perplexity for \textbf{Random} model predictions}
If predictions were random, the probability vector would be $\hat y_i^{(t)} = \frac {1} {|V|} $. \\~\\
Thus, we would expect that $$ PP = |V| $$. \\~\\
For the given vocabulary sizes, the Cross Entropy cost would be $$ J_{CE} =  log |V| $$
\begin{itemize}
\item If $|V| = 2000$, the Cross-Entropy loss would be $  \log 2000 =  10.965784$
\item If $|V| = 10000$, the Cross-Entropy loss would be $ \log 10000 = 13.287712 $
\end{itemize}


\subsection*{B. Gradients}
 We will notate the linear inputs of the activation functions as $$ z_2 = h^{(t-1)}\cdot H + e^{(t)}\cdot I+ b_1 $$ $$ z_3 = h^{(t)} \cdot U + b_2$$

\subsubsection*{Output Layer}
$$
	\frac {\partial J}{\partial b_2} \Big|_t = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial b_2} = (\hat y - y) \cdot 1 = \hat y - y = \delta_{(3)}^{(t)}
$$

$$
	\frac {\partial J}{\partial U} \Big|_t = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial U} = (\hat y - y) \otimes h = \delta_{(3)}^{(t)} \otimes h^{(t)}
$$

\subsubsection*{Hidden Layer}
$$
	\frac {\partial J}{\partial b_1} \Big|_t= \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial h} \cdot \frac {\partial h}{\partial z_2} \cdot \frac {\partial z_2}{\partial b_1} = \delta_{(3)}^{(t)} \cdot U^{T} \cdot \sigma^{'}(z_2) \cdot 1= \delta_{(3)}^{(t)} \cdot U^{T} \cdot \sigma^{'}(z_2) = \delta_{(2)}^{(t)}
$$0

$$
	\frac {\partial J}{\partial I}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial I} = \delta_{(2)}^{(t)} \otimes {e^{(t)}}
$$

$$
	\frac {\partial J}{\partial H}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial H} = \delta_{(2)}^{(t)} \otimes {h^{(t-1)}}
$$

\subsubsection*{Embeddings}
$$
	\frac {\partial J}{\partial L_{x^{(t)}}}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac{\partial z_2}{\partial e^{(t)}}\cdot \frac {\partial e^{(t)}}{\partial  L_{x^{(t)}}} = \delta_{(2)}^{(t)} \cdot I^{T} \cdot 1 = \delta_{(1)}^{(t)}
$$

\subsubsection*{Previous HIdden Layer}
$$
	\frac {\partial J}{\partial h^{(t-1)}}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial h^{(t-1)}} = \delta_{(2)}^{(t)} \cdot H^{T} = \delta^{(t-1)}
$$

\subsection*{C. RNN Temporal Structure}
\subsubsection*{Unrolled network for 3 time-steps}
\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1.25cm
  }
}

\begin{figure}[!h]
\centering
\begin{tikzpicture}

\foreach \m [count=\y] in {-3, -2, -1, 0, 1}
  \node [every neuron/.try, neuron \m/.try] (h-\m) at (1.75 * \y - 1.75 , 0) {
	\ifthenelse{\m=0}{$h^{(t)}$}{\ifthenelse{\m>0}{$h^{(t+\m)}$}{$h^{(t \m)}$}}
};

\foreach \m [count=\y] in {-2, -1, 0}
  \node [every neuron/.try, neuron \m/.try] (x-\m) at (1.75 * \y, -2) {
	\ifthenelse{\m=0}{$x^{(t)}$}{$x^{(t \m)}$}
};

\foreach \m [count=\y] in {-2, -1, 0}
  \node [every neuron/.try, neuron \m/.try] (y-\m) at (1.75 * \y, +2) {
	\ifthenelse{\m=0}{$\hat y^{(t)}$}{$\hat y^{(t \m)}$}
};


\draw [->] (h--3) -- (h--2);
\draw [->] (h--2) -- (h--1);
\draw [->] (h--1) -- (h-0);
\draw [->] (h-0) -- (h-1);

\foreach \i in {-2, -1, 0}
    \draw [->] (x-\i) -- (h-\i);

\foreach \i in {-2, -1, 0}
    \draw [->] (h-\i) -- (y-\i);

\end{tikzpicture}
\caption{Unrolled Network}
\end{figure}
\ 
\\~\\

\subsubsection*{Gradients, Through Time}

$$
	\frac {\partial J}{\partial b_1}\Big|_t = \frac {\partial J}{\partial h^{(t-1)}} \cdot \frac{\partial h^{(t-1)}}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial b_1} = \delta^{(t-1)} \otimes \sigma^{'}\left(z_2^{(t-1)}\right) \cdot 1 = \delta_{H}^{(t-1)}
$$

$$
	\frac {\partial J}{\partial H} \Big|_t= \frac {\partial J}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial  H} = \delta_{H}^{(t-1)} \otimes {h^{(t-2)}}^{T}
$$

$$
	\frac {\partial J}{\partial I} \Big|_t= \frac {\partial J}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial  I} = \delta_{H}^{(t-1)} \otimes {e^{(t-1)}}^{T}
$$

$$
	\frac {\partial J}{\partial L_{x^{(t-1)}}} = \frac {\partial J}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial  L_{x^{(t-1)}}} = \delta_{H}^{(t-1)} \otimes I^{T}
$$


\subsection*{D. Asymptotic Constraints}
\subsubsection*{Feed-Forward (Single Step and Multiple Steps)}
$$O(|V|D_h + dD_h+D_h^{2})$$

\subsubsection*{Back Propogation (Single Step)}
$$O(|V|D_h + dD_h+D_h^{2})$$

\subsubsection*{Back Propogation (Multiple Steps)}
$$O(\tau(|V|D_h + dD_h+D_h^{2}))$$

\subsubsection*{Slowest Step}
Assuming that $|V| >> D_h$, the slow step would be the $O(|V|D_h)$ term - which is the computation of the probability ditribution over all next words.


\subsection*{E. Results}
\subsubsection*{Hyperparameter Analysis}
While using the configuration
\begin{lstlisting}
  batch_size = 64
  embed_size = 75
  hidden_size = 150
  num_steps = 15
  max_epochs = 16
  early_stopping = 2
  dropout = 0.98999999999999999
  lr = 0.001
\end{lstlisting}

We acheived a Perplexity score of $165.895$

\subsection*{F. \textbf{RNNLM} Generated Sentences }
The following are generated sentences from our best model
\begin{itemize}
\item ``jail <unk> make a total effect and it is both assurance assurance of america which it likes to u.s. apples are <unk> <eos>"
\item ``japan N children want to consider a line-item veto process later <eos>"
\item ``last year budget deficit that making possible silver 's fiscal year at \$ N a share which had a big number of taxation <eos>"
\end{itemize}


\end{document}