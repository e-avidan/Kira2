\documentclass{article}      
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage{ifthen}

\begin{document}

\title{%
  236605: Assignment \#2 \\
  \large RNN for NER and LM \\
    Fall Semester '17-'18}

\author{
  Avidan, Eyal \\
  \texttt{205796469}
  \and
  Goaz, Or \\
  \texttt{307950113}
}

\maketitle

\section*{1. Softmax}
\subsection*{C. Explanation of TensorFlow Components}
TensorFlow defines two objects used in the \textbf{Model} class
\begin{itemize}
\item \emph{Placeholder Variables} - nodes that expect a value to be provided from the outside - Inputs!
\item \emph{Feed Dictionaries} - a dictionary of data, provided when running a session over the graph, that defines the values of \emph{Placeholder Variables}
\end{itemize}

\subsection*{E. Why we like \emph{TensorFlow} and its automatic differentiation}
Since TensorFlow performs differentation automatically (probably similarly to the way we checked the gradient in the previous exercise), we don't need to specify them - and furthermore, we don't have to calculate them for every change we make to our model, allowing us to play with the computational graph at ease


\section*{2. Deep Networks for \textbf{NER}}
\subsection*{A. Gradients}
Before delving into the gradients of the network, we'll calculate the gradient of the activation function by its parameter
$$
	\frac {\partial tanh(z)} {\partial z} = 2 \cdot \frac {\partial} {\partial z} \cdot \sigma (2z) = 4 \cdot \sigma(2z)\cdot (1- \sigma(2z))
$$

Playinng a bit with the numbers, we can acheive
$$
	[ 2 \cdot \sigma(2z)] \cdot [2 - 2\sigma(2z)]= [(2\sigma(2z) -1) + 1] \cdot -[(2\sigma(2z) -1) - 1] = -[tanh(z) + 1][tanh(z)-1] = -(tanh^2(z) - 1)
$$

Meaning that
$$
	\frac {\partial tanh(z)} {\partial z} = 1 - tanh^2(z)
$$

In addition to this, we will note some facts and notations:
\begin{itemize}
\item When using cross-entropy cost and a softmax activation, the gradient of the cost function by the softmax parameter is $ \frac {\partial J}{\partial \theta} = \hat y - y$ (Proven in HW1)
\item We will notate the linear inputs of the activation functions as $$ z_2 = x^{(t)}\cdot W + b_1 $$ $$ z_3 = h \cdot U + b_2$$
\end{itemize}

\subsubsection*{Output Layer}
$$
	\frac {\partial J}{\partial b_2} = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial b_2} = (\hat y - y) \cdot 1 = \hat y - y = \delta_{(3)}
$$

$$
	\frac {\partial J}{\partial U} = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial U} = (\hat y - y) \otimes h = \delta_{(3)} \otimes h
$$

\subsubsection*{Hidden Layer}
$$
	\frac {\partial J}{\partial b_1} = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial h} \cdot \frac {\partial h}{\partial z_2} \cdot \frac {\partial z_2}{\partial b_1} = \delta_{(3)} \cdot U^{T} \cdot tanh^{'}(z_2) \cdot 1= \delta_{(3)} \cdot U^{T} \cdot tanh^{'}(z_2) = \delta_{(2)}
$$

$$
	\frac {\partial J}{\partial W} = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial W} = \delta_{(2)} \otimes {x^{(t)}}^{T}
$$

\subsubsection*{Words}
?????????????????
$$
	\frac {\partial J}{\partial L_i} = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial L_i} = \delta_{(2)} \cdot {W}^{T}
$$

\subsection*{B. Gradients, with Regularization}
Since they don't appear in the regularization factor,
$$
	\frac {\partial J_{reg}}{\partial L_i} = \frac {\partial J_{reg}}{\partial b_2}  = \frac {\partial J_{reg}}{\partial b_1}  = 0
$$

So their gradients remain the same. \\~\\
As for the weights

$$
	\frac {\partial J_{full}}{\partial U} = \frac {\partial J}{\partial U} + \frac {\partial J_{reg}}{\partial U} = \frac {\partial J}{\partial U} +  \lambda \cdot U
$$

$$
	\frac {\partial J_{full}}{\partial W} = \frac {\partial J}{\partial W} + \frac {\partial J_{reg}}{\partial W} = \frac {\partial J}{\partial W} +  \lambda \cdot W
$$

\subsection*{C. Results}
\subsubsection*{Test Set Labels}
dsadsa

\subsubsection*{Hyperparameter Analysis}
Quite a few things were apparent while testing different combinations of hyperparameters:
\begin{itemize}
\item Since most labels were objects, the model easily converged to a local minima with the default \emph{learning rate}, so we have to increase it (but not too much)
\item There weren't so much data, so in order to prevent overfitting we tempered with the hidden layer size (less parameters to learn)
\item Dropout was very important, as it made the network perform similarly on test and dev
\end{itemize}



\section*{3. Recurrent Neural Networks for \emph{LM}}
\subsection*{A. Perplexity}
\subsubsection*{Deriviation from Cross-Entropy}
We will raise an exponent to the power of the Cross-Entropy cost
$$
	e^{J^{(t)}} = e^{-\sum_{i=1}^{|V|}{y_i^{(t)}}\cdot \log \left(\hat y_i^{(t)}\right)} = \frac {1} {e^{\sum_{i=1}^{|V|}{y_i^{(t)}}\cdot \log \left(\hat y_i^{(t)}\right)}} =
$$
$$
	= \frac {1} {\prod_{i=1}^{|V|}{e^{ \ln \left({\hat y_i}^{(t)} \right)^{y_i^{(t)}}}}} = \frac {1} {\prod_{i=1}^{|V|}{\left({\hat y_i}^{(t)} \right)^{y_i^{(t)}}}}= 
$$

Since $y_i^{(t)}$ is one-hot, we can replace the product with a summation, so that
$$
	e^{J^{(t)}} = \frac {1} {\sum_{i=1}^{|V|}{y_i^{(t)}\cdot {\hat y_i}^{(t)}}} = PP^{(t)}\left(y^{(t)}, {\hat y}^{(t)}\right)
$$

\subsubsection*{Minimizing Mean Cross-Entropy and Mean Perplexity}
Using the same exponent from before:
$$
	e^{\mu \left(J^{(t)}\right)} = e^{\frac {1}{N} \cdot \sum_t {J^{(t)}}} = \sqrt[N]{e^{\sum_t {J^{(t)}}}} = \sqrt[N]{\prod_t {e^{J^{(t)}}}} = 
$$
$$
	= \sqrt[N]{\prod_t {PP^{(t)} \left(y^{(t)}, {\hat y}^{(t)}\right)}}
$$

Thus, meanimizing the mean-cross entropy will also minimize the Perplexity, since we only raised an exponent by it, which is a monotone operation.

\subsubsection*{Cross-Entropy loss and Perplexity for \textbf{Random} model predictions}
If predictions were random, the probability vector would be $\hat y_i^{(t)} = \frac {1} {|V|} $. \\~\\
Thus, we would expect that $$ PP = |V| $$. \\~\\
For the given vocabulary sizes, the Cross Entropy cost would be $$ J_{CE} = |V| \cdot - \log \frac {1} {|V|} = |V| \cdot log |V| $$
\begin{itemize}
\item If $|V| = 2000$, the Cross-Entropy loss would be $ 2000 \cdot \log 2000 = 6602.05999133 $
\item If $|V| = 10000$, the Cross-Entropy loss would be $ 10000 \cdot \log 10000 = 40000 $
\end{itemize}


\subsection*{B. Gradients}
 We will notate the linear inputs of the activation functions as $$ z_2 = h^{(t-1)}\cdot H + e^{(t)}\cdot I+ b_1 $$ $$ z_3 = h^{(t)} \cdot U + b_2$$

\subsubsection*{Output Layer}
$$
	\frac {\partial J}{\partial b_2} \Big|_t = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial b_2} = (\hat y - y) \cdot 1 = \hat y - y = \delta_{(3)}^{(t)}
$$

$$
	\frac {\partial J}{\partial U} \Big|_t = \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial U} = (\hat y - y) \otimes h = \delta_{(3)}^{(t)} \otimes h^{(t)}
$$

\subsubsection*{Hidden Layer}
$$
	\frac {\partial J}{\partial b_1} \Big|_t= \frac {\partial J}{\partial z_3} \cdot \frac {\partial z_3}{\partial h} \cdot \frac {\partial h}{\partial z_2} \cdot \frac {\partial z_2}{\partial b_1} = \delta_{(3)}^{(t)} \cdot U^{T} \cdot \sigma^{'}(z_2) \cdot 1= \delta_{(3)}^{(t)} \cdot U^{T} \cdot \sigma^{'}(z_2) = \delta_{(2)}^{(t)}
$$0

$$
	\frac {\partial J}{\partial I}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial I} = \delta_{(2)}^{(t)} \otimes {e^{(t)}}
$$

$$
	\frac {\partial J}{\partial H}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial H} = \delta_{(2)}^{(t)} \otimes {h^{(t-1)}}
$$

\subsubsection*{Embeddings}
$$
	\frac {\partial J}{\partial L_{x^{(t)}}}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac{\partial z_2}{\partial e^{(t)}}\cdot \frac {\partial e^{(t)}}{\partial  L_{x^{(t)}}} = \delta_{(2)}^{(t)} \cdot I^{T} \cdot 1 = \delta_{(1)}^{(t)}
$$

\subsubsection*{Previous HIdden Layer}
$$
	\frac {\partial J}{\partial h^{(t-1)}}\Big|_t = \frac {\partial J}{\partial z_2} \cdot \frac {\partial z_2}{\partial h^{(t-1)}} = \delta_{(2)}^{(t)} \cdot H^{T} = \delta^{(t-1)}
$$

\subsection*{C. RNN Temporal Structure}
\subsubsection*{Unrolled network for 3 time-steps}
\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=1.25cm
  }
}

\begin{figure}[!h]
\centering
\begin{tikzpicture}

\foreach \m [count=\y] in {-3, -2, -1, 0, 1}
  \node [every neuron/.try, neuron \m/.try] (h-\m) at (1.75 * \y - 1.75 , 0) {
	\ifthenelse{\m=0}{$h^{(t)}$}{\ifthenelse{\m>0}{$h^{(t+\m)}$}{$h^{(t \m)}$}}
};

\foreach \m [count=\y] in {-2, -1, 0}
  \node [every neuron/.try, neuron \m/.try] (x-\m) at (1.75 * \y, -2) {
	\ifthenelse{\m=0}{$x^{(t)}$}{$x^{(t \m)}$}
};

\foreach \m [count=\y] in {-2, -1, 0}
  \node [every neuron/.try, neuron \m/.try] (y-\m) at (1.75 * \y, +2) {
	\ifthenelse{\m=0}{$\hat y^{(t)}$}{$\hat y^{(t \m)}$}
};


\draw [->] (h--3) -- (h--2);
\draw [->] (h--2) -- (h--1);
\draw [->] (h--1) -- (h-0);
\draw [->] (h-0) -- (h-1);

\foreach \i in {-2, -1, 0}
    \draw [->] (x-\i) -- (h-\i);

\foreach \i in {-2, -1, 0}
    \draw [->] (h-\i) -- (y-\i);

\end{tikzpicture}
\caption{Unrolled Network}
\end{figure}
\ 
\\~\\

\subsubsection*{Gradients, Through Time}

$$
	\frac {\partial J}{\partial b_1}\Big|_t = \frac {\partial J}{\partial h^{(t-1)}} \cdot \frac{\partial h^{(t-1)}}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial b_1} = \delta^{(t-1)} \otimes \sigma^{'}\left(z_2^{(t-1)}\right) \cdot 1 = \delta_{H}^{(t-1)}
$$

$$
	\frac {\partial J}{\partial H} \Big|_t= \frac {\partial J}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial  H} = \delta_{H}^{(t-1)} \otimes {h^{(t-2)}}^{T}
$$

$$
	\frac {\partial J}{\partial I} \Big|_t= \frac {\partial J}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial  I} = \delta_{H}^{(t-1)} \otimes {e^{(t-1)}}^{T}
$$

$$
	\frac {\partial J}{\partial L_{x^{(t-1)}}} = \frac {\partial J}{\partial z_2^{(t-1)}} \cdot \frac{\partial z_2^{(t-1)}}{\partial  L_{x^{(t-1)}}} = \delta_{H}^{(t-1)} \otimes I^{T}
$$


\subsection*{D. Asymptotic Constraints}
\subsubsection*{Feed-Forward (Single Step)}
dsa

\subsubsection*{Back Propogation (Single Step)}
dsa

\subsubsection*{Feed-Forward (Multiple Steps)}
TODO - is this necessary??? dsa

\subsubsection*{Back Propogation (Multiple Steps)}
dsa

\subsubsection*{Slowest Steps}
dsa


\subsection*{E. Results}
\subsubsection*{Test Set Results}
dsadsa

\subsubsection*{Hyperparameter Analysis}
dsadsa


\subsection*{F. \textbf{RNNLM} Generated Sentences }
dsadsa


\end{document}